<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><title>Hexo</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.3.0'
} </script><meta name="generator" content="Hexo 6.3.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84"><span class="toc-number">1.</span> <span class="toc-text">一、实验目的</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%8E%9F%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">二、实验原理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9%E5%92%8C%E6%AD%A5%E9%AA%A4"><span class="toc-number">3.</span> <span class="toc-text">三、实验内容和步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9"><span class="toc-number">3.1.</span> <span class="toc-text">(1)实验内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4"><span class="toc-number">3.2.</span> <span class="toc-text">(2)实验步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%9B%B8%E5%85%B3%E5%BA%93%E7%9A%84%E5%AF%BC%E5%85%A5"><span class="toc-number">3.2.1.</span> <span class="toc-text">1. 相关库的导入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-number">3.2.2.</span> <span class="toc-text">2.预处理和数据分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%9F%A5%E7%9C%8B"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">2.1 导入数据与查看</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E6%95%B0%E6%8D%AE%E7%BB%B4%E5%BA%A6"><span class="toc-number">3.2.2.2.</span> <span class="toc-text">2.2 数据维度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E6%95%B0%E6%8D%AE%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">3.2.2.3.</span> <span class="toc-text">2.3 数据基本信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E6%9F%A5%E7%9C%8B"><span class="toc-number">3.2.2.4.</span> <span class="toc-text">2.4 缺失值处查看</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E6%83%85%E5%86%B5"><span class="toc-number">3.2.2.5.</span> <span class="toc-text">2.5 查看数据分布情况</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">3.2.2.6.</span> <span class="toc-text">2.6 数据可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Target-Plot"><span class="toc-number">3.2.2.6.1.</span> <span class="toc-text">Target Plot</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Univariate-Plots"><span class="toc-number">3.2.2.6.2.</span> <span class="toc-text">Univariate Plots</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Multivariate-Plots"><span class="toc-number">3.2.2.6.3.</span> <span class="toc-text">Multivariate Plots</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-7-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">3.2.2.7.</span> <span class="toc-text">2.7 特征工程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.2.3.</span> <span class="toc-text">3. 机器学习模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.2.3.1.</span> <span class="toc-text">3.1 划分数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89"><span class="toc-number">3.2.3.2.</span> <span class="toc-text">3.2 逻辑回归（Logistic Regression）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%88Decision-Tree-Classifier%EF%BC%89"><span class="toc-number">3.2.3.3.</span> <span class="toc-text">3.3 决策树分类器（Decision Tree Classifier）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92%E5%99%A8%EF%BC%88Random-Forest-Regressor%EF%BC%89"><span class="toc-number">3.2.3.4.</span> <span class="toc-text">3.4 随机森林回归器（Random Forest Regressor）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-XGBoost%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%88XGBoost-Classifier%EF%BC%89"><span class="toc-number">3.2.3.5.</span> <span class="toc-text">3.5 XGBoost分类器（XGBoost Classifier）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88Support-Vector-Machines%EF%BC%89"><span class="toc-number">3.2.3.6.</span> <span class="toc-text">3.6 支持向量机（Support Vector Machines）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">4.</span> <span class="toc-text">四、实验结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%AE%9E%E9%AA%8C%E5%88%86%E6%9E%90"><span class="toc-number">5.</span> <span class="toc-text">五、实验分析</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202307311922384.jpg"></div><div class="author-info__name text-center">John Doe</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">53</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">3</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">39</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Hexo</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right"></span></div><div id="post-info"><div id="post-title">No title</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-06-04</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">数据挖掘</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/">实验报告</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="一、实验目的"><a href="#一、实验目的" class="headerlink" title="一、实验目的"></a>一、实验目的</h1><p>(1)目的是建立一个模型，可以根据蘑菇的特征预测蘑菇的食用性。<br>(2) 写出实验报告。</p>
<h1 id="二、实验原理"><a href="#二、实验原理" class="headerlink" title="二、实验原理"></a>二、实验原理</h1><p>(1)支持向量机（SVM）： 支持向量机是一种监督学习算法，用于二分类和多分类问题。在手写体识别任务中，S和VM可以被用来将手写数字图像分为不同的类别。SVM通过寻找一个最优的超平面来将不同类别的数据点分开。对于手写数字识别，每个图像被表示为一组特征向量，并与相应的标签（数字类别）相关联。SVM通过学习一个决策边界，使得不同类别的图像在特征空间中被最大化地分离。  </p>
<p>(2)K最近邻算法（K Nearest Neighbors）： K最近邻算法是一种基于实例的学习方法，用于分类和回归问题。对于手写体识别任务，K最近邻算法可以用于根据与目标图像最相似的K个训练样本的标签来预测目标图像的类别。算法通过计算目标图像与所有训练图像之间的距离（如欧氏距离）来确定最相似的训练样本，然后根据K个最相似样本的标签进行投票来确定目标图像的类别。   </p>
<p>(3) 随机森林分类器（Random Forest Classifier）： 随机森林是一种集成学习方法，它由多个决策树组成。每个决策树都是基于不同的训练样本和特征子集构建的。在手写体识别任务中，随机森林分类器可以通过将图像的特征输入到每个决策树中，并将决策树的预测结果进行投票来确定图像的类别。随机森林具有良好的泛化能力和抗过拟合能力，并且在处理高维特征空间和大规模数据集时表现良好</p>
<h1 id="三、实验内容和步骤"><a href="#三、实验内容和步骤" class="headerlink" title="三、实验内容和步骤"></a>三、实验内容和步骤</h1><h2 id="1-实验内容"><a href="#1-实验内容" class="headerlink" title="(1)实验内容"></a>(1)实验内容</h2><ol>
<li>用python编写程序，通过SVM、K Nearest Neighbors、Random Forest Classifier分别实现根据蘑菇的特征预测蘑菇的食用性</li>
<li>分别评估模型</li>
</ol>
<h2 id="2-实验步骤"><a href="#2-实验步骤" class="headerlink" title="(2)实验步骤"></a>(2)实验步骤</h2><ol>
<li>相关库的导入</li>
<li>预处理和数据分析</li>
<li>机器学习模型</li>
</ol>
<p>具体步骤：</p>
<h3 id="1-相关库的导入"><a href="#1-相关库的导入" class="headerlink" title="1. 相关库的导入"></a>1. 相关库的导入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os <span class="comment">#paths to file</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing</span></span><br><span class="line"><span class="keyword">import</span> warnings<span class="comment"># warning filter</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ploting libraries</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment">#feature engineering</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment">#train test split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment">#metrics</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_validate</span><br><span class="line"></span><br><span class="line"><span class="comment">#cross validation</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection  <span class="keyword">import</span> cross_val_score <span class="keyword">as</span> CVS</span><br><span class="line"></span><br><span class="line"><span class="comment">#ML models</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment">#default theme and settings</span></span><br><span class="line">sns.<span class="built_in">set</span>(context=<span class="string">&#x27;notebook&#x27;</span>, style=<span class="string">&#x27;darkgrid&#x27;</span>, palette=<span class="string">&#x27;deep&#x27;</span>, font=<span class="string">&#x27;sans-serif&#x27;</span>, font_scale=<span class="number">1</span>, color_codes=<span class="literal">False</span>, rc=<span class="literal">None</span>)</span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#warning handle</span></span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;always&quot;</span>)</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-预处理和数据分析"><a href="#2-预处理和数据分析" class="headerlink" title="2.预处理和数据分析"></a>2.预处理和数据分析</h3><h4 id="2-1-导入数据与查看"><a href="#2-1-导入数据与查看" class="headerlink" title="2.1 导入数据与查看"></a>2.1 导入数据与查看</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Mushroom_df = pd.read_csv(Mushroom_path)</span><br><span class="line">Mushroom_df.head()</span><br></pre></td></tr></table></figure>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042040817.png" alt="image.png"></p>
<h4 id="2-2-数据维度"><a href="#2-2-数据维度" class="headerlink" title="2.2 数据维度"></a>2.2 数据维度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Data Shape (row, col): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(Mushroom_df.shape)</span><br></pre></td></tr></table></figure>
<p>输出：<br><code>Data Shape (row, col): (8124, 23)</code></p>
<h4 id="2-3-数据基本信息"><a href="#2-3-数据基本信息" class="headerlink" title="2.3 数据基本信息"></a>2.3 数据基本信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mushroom_df.info()</span><br></pre></td></tr></table></figure>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042043188.png" alt="image.png"></p>
<h4 id="2-4-缺失值处查看"><a href="#2-4-缺失值处查看" class="headerlink" title="2.4 缺失值处查看"></a>2.4 缺失值处查看</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#missing values</span></span><br><span class="line">Mushroom_df.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042045622.png" alt="image.png"><br>可见，该数据集并无缺失值。</p>
<h4 id="2-5-查看数据分布情况"><a href="#2-5-查看数据分布情况" class="headerlink" title="2.5 查看数据分布情况"></a>2.5 查看数据分布情况</h4><p>代码将遍历每一列并打印该列的值分布情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">M_cols = Mushroom_df.columns.to_list()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Value Distribution:\n&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> M_cols:</span><br><span class="line">    <span class="built_in">print</span>(col,<span class="string">&quot;\n&quot;</span>,Mushroom_df[col].value_counts(),<span class="string">&quot;\n\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042047355.png" alt="image.png"></p>
<h4 id="2-6-数据可视化"><a href="#2-6-数据可视化" class="headerlink" title="2.6 数据可视化"></a>2.6 数据可视化</h4><h5 id="Target-Plot"><a href="#Target-Plot" class="headerlink" title="Target Plot"></a>Target Plot</h5><p><strong>Target Plot</strong>: 这是一种数据可视化技术，通常用于显示目标变量（也称为响应变量或因变量）的分布。帮助理解数据的平衡或不平衡，并可能影响选择的建模策略。</p>
<p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">total = <span class="built_in">float</span>(<span class="built_in">len</span>(Mushroom_df[M_cols[<span class="number">0</span>]]))</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;darkgrid&quot;</span>)</span><br><span class="line">ax = sns.countplot(Mushroom_df[M_cols[<span class="number">0</span>]])</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> ax.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    ax.text(p.get_x()+p.get_width()/<span class="number">2.</span>,height + <span class="number">3</span>,<span class="string">&#x27;&#123;:1.2f&#125;&#x27;</span>.<span class="built_in">format</span>(height/total),ha=<span class="string">&quot;center&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Target Plot&quot;</span>, fontsize = <span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><code>total = float(len(Mushroom_df[M_cols[0]]))</code>: 这行代码计算了目标变量的总实例数。</li>
<li><code>plt.figure(figsize=(6,6))</code>: 这行代码设置了图形的大小。</li>
<li><code>sns.set(style=&quot;darkgrid&quot;)</code>: 这行代码设置了 seaborn 图形的样式。</li>
<li><code>ax = sns.countplot(Mushroom_df[M_cols[0]])</code>: 这行代码创建了一个 countplot，显示了目标变量的每个类别的实例数量。</li>
<li><code>for p in ax.patches: ...</code>: 这个循环在每个条形图上方添加了一个文本，显示了该类别的实例数量占总实例数的百分比。</li>
<li><code>plt.title(&quot;Target Plot&quot;, fontsize = 20)</code>: 这行代码设置了图形的标题和字体大小。</li>
<li><code>plt.show()</code>: 这行代码显示了图形。</li>
</ol>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042100812.png" alt="image.png"></p>
<h5 id="Univariate-Plots"><a href="#Univariate-Plots" class="headerlink" title="Univariate Plots"></a>Univariate Plots</h5><p> <strong>Univariate Plots</strong>: 单变量图是一种数据可视化技术，用于显示单个变量（特征）的分布。这可以包括直方图、箱线图、密度图等。这些图可以帮助理解变量的中心趋势（如平均值或中位数）、离散度（如范围或四分位数范围）、形状（如偏度或峰度）以及任何潜在的异常值。</p>
<p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> M_cols[<span class="number">1</span>:]:</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">4</span>))</span><br><span class="line">    sns.countplot(x=col , data=Mushroom_df ,palette=<span class="string">&#x27;icefire&#x27;</span>)</span><br><span class="line">    plt.title(col, fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;% of total:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">round</span>((Mushroom_df[col].value_counts()/Mushroom_df.shape[<span class="number">0</span>]),<span class="number">4</span>)*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><code>for col in M_cols[1:]: ...</code>: 这个循环对 <code>M_cols</code> 列表中的每个元素（除了第一个元素）执行以下操作。</li>
<li><code>plt.figure(figsize=(10,4))</code>: 这行代码设置了图形的大小。</li>
<li><code>sns.countplot(x=col , data=Mushroom_df ,palette=&#39;icefire&#39;)</code>: 这行代码创建了一个 countplot，显示了给定特征的每个类别的实例数量。</li>
<li><code>plt.title(col, fontsize=14)</code>: 这行代码设置了图形的标题和字体大小。</li>
<li><code>plt.show()</code>: 这行代码显示了图形。</li>
<li><code>print(&quot;% of total:&quot;)</code>: 这行代码打印了一个字符串，表示接下来的数字是总数的百分比。</li>
<li><code>print(round((Mushroom_df[col].value_counts()/Mushroom_df.shape[0]),4)*100)</code>: 这行代码计算了每个类别的实例数量占总实例数的百分比，并打印了结果。</li>
</ol>
<p>上述代码创建了一系列的单变量图，显示每个特征的分布，并打印了每个类别的实例数量占总实例数的百分比，以下列出部分输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042106429.png" alt="image.png"></p>
<p><strong>单变量分析（Unvariate Analisys）：</strong></p>
<ol>
<li><strong><code>cap-shape</code></strong> - 数据集中的大多数蘑菇都有凸形（45%）或平坦（38.8%）的帽子。</li>
<li><strong><code>cap-surface</code></strong> - 最常见的帽子表面是鳞状的（39.93%），几乎没有沟槽（0.05%）。</li>
<li><strong><code>cap-color</code></strong> - 数据集中超过一半的蘑菇有棕色（28.11%）或灰色（22.65%）的帽子颜色，我们还有8种其他颜色出现的频率较低。</li>
<li><strong><code>bruises</code></strong> - 数据集中的大多数蘑菇都没有瘀伤（58.44%）。</li>
<li><strong><code>odor</code></strong> - 最常见的气味（超过70%）要么没有气味（43.43%），要么有恶臭（26.59%）。</li>
<li><strong><code>gill-attachment</code></strong> - 我们的数据中几乎所有的蘑菇都有自由的菌褶附着（约97.5%），所以这一列在我们的分析中几乎是无用的。</li>
<li><strong><code>gill-spacing</code></strong> - 我们数据集中超过85%的蘑菇有紧密的菌褶间距，所以它在我们的分析中几乎没有影响。</li>
<li><strong><code>gill-size</code></strong> - 我们数据集中的大多数蘑菇有宽的（69.08%）菌褶大小，其他的是窄的。</li>
<li><strong><code>gill-color</code></strong> - 我们数据集中超过一半的蘑菇有浅黄色（21.27%）、粉红色（18.37%）或白色（22.65%）的菌褶颜色，我们还有9种其他颜色出现的频率较低。</li>
<li><strong><code>stalk-shape</code></strong> - 我们数据集中的大多数蘑菇有锥形的（56.72%），其他的是扩大的。</li>
<li><strong><code>stalk-root</code></strong> - 最常见的柄根（超过75%）要么是球状的（46.48%）要么是丝状的（30.53%）。</li>
<li><strong><code>stalk-surface-above-ring</code></strong> - 最常见的环上柄表面（超过90%）要么是光滑的（63.71%）要么是丝状的（29.2%）。</li>
<li><strong><code>stalk-surface-below-ring</code></strong> - 最常见的环下柄表面（几乎90%）要么是光滑的（60.76%）要么是丝状的（28.36%），与<code>stalk-surface-above-ring</code>几乎没有区别。</li>
<li><strong><code>stalk-color-above-ring</code></strong> - 我们数据集中超过一半的蘑菇有白色（54.95%）的柄颜色，我们还有8种其他颜色出现的频率较低。</li>
<li><strong><code>stalk-color-below-ring</code></strong> - 我们数据集中超过一半的蘑菇有白色（53.96%）的柄颜色，我们还有8种其他颜色出现的频率较低，与<code>stalk-color-above-ring</code>几乎没有区别。</li>
<li><strong><code>veil-type</code></strong> - 所有蘑菇的面纱类型都是部分的，所以这一列在我们的分析中几乎是无用的。</li>
<li><strong><code>veil-color</code></strong> - 几乎所有的蘑菇面纱颜色都是白色（97.54%），所以这一列在我们的分析中几乎是无用的。</li>
<li><strong><code>ring-number</code></strong> - 几乎所有的蘑菇环数都是一个（92.17%），所以这一列在我们的分析中几乎是无用的。</li>
<li><strong><code>ring-type</code></strong> - 我们数据集中超过98%的蘑菇有吊坠（48.84%）、短暂的（34.17%）或大的（15.95%）环类型。</li>
<li><strong><code>spore-print-color</code></strong> - 最常见的颜色是白色（29.39%）、棕色（24.22%）、黑色（23.04%）和巧克力色（20.09%），其他颜色不重要。</li>
<li><strong><code>population</code></strong> - 最常见的种群（超过70%）要么是多个（49.73%）要么是孤立的（21.07%）。</li>
<li><strong><code>habitat</code></strong> - 最常见的栖息地（超过65%）要么是森林（38.75%）要么是草地（26.44%）。</li>
</ol>
<h5 id="Multivariate-Plots"><a href="#Multivariate-Plots" class="headerlink" title="Multivariate Plots"></a>Multivariate Plots</h5><p><strong>Multivariate Plots</strong>: 多变量图是一种数据可视化技术，用于显示两个或更多变量之间的关系。这可以包括散点图、相关矩阵、平行坐标图等。这些图可以帮助你理解变量之间的关系，例如它们是否有相关性或是否存在任何群集或异常值。</p>
<p>在本处，将对每个属性，结合目标类别（p，e）进行绘制柱状图。</p>
<p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> M_cols[<span class="number">1</span>:]: </span><br><span class="line">    plt.figure(figsize=(<span class="number">30</span>,<span class="number">20</span>))</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    sns.countplot(x=col ,hue=<span class="string">&#x27;class&#x27;</span>, data=Mushroom_df ,palette=<span class="string">&#x27;viridis_r&#x27;</span>)</span><br><span class="line">    plt.xlabel(col, fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><code>for col in M_cols[1:]: ...</code>: 这个循环对 <code>M_cols</code> 列表中的每个元素（除了第一个元素）执行以下操作。</li>
<li><code>plt.figure(figsize=(30,20))</code>: 这行代码设置了图形的大小。</li>
<li><code>plt.subplot(2,3,1)</code>: 这行代码创建了一个子图，它是一个2行3列的网格中的第一个图。</li>
<li><code>sns.countplot(x=col ,hue=&#39;class&#39;, data=Mushroom_df ,palette=&#39;viridis_r&#39;)</code>: 这行代码创建了一个 countplot，显示了给定特征的每个类别的实例数量，并使用颜色来区分目标变量的类别。</li>
<li><code>plt.xlabel(col, fontsize=20)</code>: 这行代码设置了图形的 x 轴标签和字体大小。</li>
<li><code>plt.legend(loc=&#39;upper left&#39;)</code>: 这行代码添加了一个图例，并设置了它的位置。</li>
</ol>
<p>上述代码创建了一系列的多变量条形图，显示每个特征与目标变量之间的关系，以下列出部分输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042128145.png" alt="image.png"></p>
<p>特征与目标变量关系的分析<br>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ pd.pivot_table(Mushroom_df, index=[col,<span class="string">&quot;class&quot;</span>], aggfunc = &#123;col:np.count_nonzero&#125;) <span class="keyword">for</span> col <span class="keyword">in</span> M_cols[<span class="number">1</span>:]]</span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><code>for col in M_cols[1:]: ...</code>: 这个循环对 <code>M_cols</code> 列表中的每个元素（除了第一个元素）执行以下操作。</li>
<li><code>pd.pivot_table(Mushroom_df, index=[col,&quot;class&quot;], aggfunc = &#123;col:np.count_nonzero&#125;)</code>: 这行代码创建了一个透视表，其中行索引是给定特征和目标变量的类别，聚合函数是非零计数。</li>
</ol>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042133492.png" alt="image.png"></p>
<p><strong>特征与目标变量分析（Multivariate Analisys）：</strong></p>
<ol>
<li><strong><code>cap-shape</code></strong> - 我们数据集中的大多数<strong>瘤状</strong>蘑菇都是有毒的。</li>
<li><strong><code>cap-surface</code></strong> - 大多数<strong>纤维状</strong>帽面的蘑菇是可食用的。</li>
<li><strong><code>cap-color</code></strong> - 大多数<strong>白色</strong>帽子的蘑菇是可食用的，而大多数<strong>黄色</strong>帽子的蘑菇是有毒的。</li>
<li><strong><code>bruises</code></strong> - <strong>有瘀伤</strong>的蘑菇通常是可食用的，而<strong>无瘀伤</strong>的蘑菇通常是相反的。</li>
<li><strong><code>odor</code></strong> - <strong>无气味</strong>的蘑菇大多是可食用的，而所有<strong>恶臭</strong>的蘑菇都是有毒的。</li>
<li><strong><code>gill-attachment</code></strong> - <strong>附着</strong>的菌褶几乎总是可食用的。</li>
<li><strong><code>gill-spacing</code></strong> - <strong>密集</strong>的菌褶几乎总是可食用的。</li>
<li><strong><code>gill-size</code></strong> - <strong>窄</strong>的菌褶大小的蘑菇几乎总是有毒的。</li>
<li><strong><code>gill-color</code></strong> - <strong>浅黄色</strong>菌褶颜色的蘑菇总是有毒的。</li>
<li><strong><code>stalk-shape</code></strong> - 在有毒或可食用方面，每个值之间的差异不大。</li>
<li><strong><code>stalk-root</code></strong> - 通常，柄根数据缺失的蘑菇通常是有毒的。</li>
<li><strong><code>stalk-surface-above-ring</code></strong> - 通常，<strong>丝状</strong>的蘑菇是有毒的，<strong>光滑</strong>的通常是可食用的。</li>
<li><strong><code>stalk-surface-below-ring</code></strong> - 与<code>stalk-surface-above-ring</code>大致相同。</li>
<li><strong><code>stalk-color-above-ring</code></strong> - 通常，<strong>白色</strong>柄色的蘑菇是可食用的，粉红色的大多数是有毒的。</li>
<li><strong><code>stalk-color-below-ring</code></strong> - 与<code>stalk-color-above-ring</code>大致相同。</li>
<li><strong><code>veil-type</code></strong> - 所有蘑菇的面纱类型都是部分的，所以这一列在我们的分析中几乎是无用的。</li>
<li><strong><code>veil-color</code></strong> - 几乎所有的蘑菇面纱颜色都是**白色 (97.54%)**，所以这一列在我们的分析中几乎是无用的。</li>
<li><strong><code>ring-number</code></strong> - 几乎所有的蘑菇环数都是**一个 (92.17%)**，所以这一列在我们的分析中几乎是无用的。</li>
<li><strong><code>ring-type</code></strong> - <strong>吊坠</strong>环型的蘑菇大多是可食用的，<strong>短暂</strong>的大多是有毒的，<strong>大</strong>环型的都是有毒的。</li>
<li><strong><code>spore-print-color</code></strong> - <strong>棕色</strong>和<strong>黑色</strong>的几乎完全是可食用的，而<strong>白色</strong>和**巧克力色 (20.09%)**的大多数是有毒的。</li>
<li><strong><code>population</code></strong> - 通常，<strong>多个</strong>种群的蘑菇大多是有毒的。</li>
<li><strong><code>habitat</code></strong> - 在<strong>森林</strong>或<strong>草地</strong>生长的蘑菇大多是可食用的。</li>
</ol>
<h4 id="2-7-特征工程"><a href="#2-7-特征工程" class="headerlink" title="2.7 特征工程"></a>2.7 特征工程</h4><p>根据以上分析，不需要以下列，因此将删除它们：“鳃附着”、“茎形状”、“环下茎表面”、“圈下茎颜色”、“面纱类型”、“纱颜色”、”环编号“。<br>所以现在将创建新的特征变量X，不包括上述列：</p>
<p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Del_cols = [<span class="string">&#x27;class&#x27;</span>,<span class="string">&#x27;gill-attachment&#x27;</span>, <span class="string">&#x27;stalk-shape&#x27;</span>, <span class="string">&#x27;stalk-surface-below-ring&#x27;</span>, <span class="string">&#x27;stalk-color-below-ring&#x27;</span>, <span class="string">&#x27;veil-type&#x27;</span>, <span class="string">&#x27;veil-color&#x27;</span>, <span class="string">&#x27;ring-number&#x27;</span>] </span><br><span class="line">X =Mushroom_df.copy()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> Del_cols:</span><br><span class="line">    X = X.drop(col, axis = <span class="number">1</span>)</span><br><span class="line">X=pd.get_dummies(X,columns=X.columns,drop_first=<span class="literal">True</span>)</span><br><span class="line">X.head(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042141037.png" alt="image.png"></p>
<p>创建目标变量y，并将其转换为相应格式（0、1）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = Mushroom_df[<span class="string">&#x27;class&#x27;</span>]</span><br><span class="line">Encoder_y=LabelEncoder()</span><br><span class="line">y = Encoder_y.fit_transform(y)</span><br><span class="line">y</span><br></pre></td></tr></table></figure>
<p>输出：<br><code>array([1, 0, 0, ..., 0, 1, 0])</code></p>
<h3 id="3-机器学习模型"><a href="#3-机器学习模型" class="headerlink" title="3. 机器学习模型"></a>3. 机器学习模型</h3><p><strong>前言：</strong><br>将“X”作为特征，将“y”作为“类”——想要预测的目标值。<br>这是一个分类问题，所以使用分类的方法。<br>训练集和测试集的遍历为75:25</p>
<p><strong>将使用的模型：</strong><br>逻辑回归（Logistic Regression）<br>决策树分类器（Decision Tree Classifier）<br>随机森林回归器（Random Forest Regressor）<br>XGBoost分类器（XGBoost Classifier）<br>支持向量机（Support Vector Machines）</p>
<p><strong>数据建模过程</strong>：<br>1.导入模型2.训练模型3.预测4.分类指标评估</p>
<p><strong>分类评分指标：</strong><br><em><strong>准确度：</strong></em><br>$$Accuracy&#x3D;\frac{TP+TN}{TP+TN+FP+FN}$$</p>
<p><strong>精确性：</strong><br>$$Precision&#x3D;\frac{TP}{TP+FP}$$<br><strong>召回度：</strong><br>$$Recall&#x3D;\frac{TP}{TP+FN}$$<br><strong>f1分数：</strong>-介于0和1之间的数字，即精度和查全率的调和平均值。<br>$$F_1&#x3D;2<em>\frac{precision</em>recall}{precison+recall}$$</p>
<h4 id="3-1-划分数据集"><a href="#3-1-划分数据集" class="headerlink" title="3.1 划分数据集"></a>3.1 划分数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.75</span>, random_state = <span class="number">101</span>)</span><br></pre></td></tr></table></figure>

<p>为了提高评估方法的准确性，需要使用交叉验证方法，对每个模型使用该方法，并构建一个分类报告，用于对模型的最终排名进行排名和确定。<br>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_val</span>(<span class="params">model_name,model,X,y,cv</span>):</span><br><span class="line">    </span><br><span class="line">    scores = CVS(model, X, y, cv=cv)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;model_name&#125;</span> Scores:&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> scores:</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">round</span>(i,<span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Average <span class="subst">&#123;model_name&#125;</span> score: <span class="subst">&#123;<span class="built_in">round</span>(scores.mean(),<span class="number">4</span>)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">index = [<span class="string">&#x27;Valid1&#x27;</span>,<span class="string">&#x27;Valid2&#x27;</span>,<span class="string">&#x27;Valid3&#x27;</span>,<span class="string">&#x27;Valid4&#x27;</span>,<span class="string">&#x27;Valid5&#x27;</span>]</span><br><span class="line">scoring = [<span class="string">&#x27;accuracy&#x27;</span>,<span class="string">&#x27;precision&#x27;</span>, <span class="string">&#x27;recall&#x27;</span>,<span class="string">&#x27;f1&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h4 id="3-2-逻辑回归（Logistic-Regression）"><a href="#3-2-逻辑回归（Logistic-Regression）" class="headerlink" title="3.2 逻辑回归（Logistic Regression）"></a>3.2 逻辑回归（Logistic Regression）</h4><p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model</span></span><br><span class="line">LR = LogisticRegression()</span><br><span class="line"><span class="comment">#cross validation</span></span><br><span class="line">LR_report = pd.DataFrame(cross_validate(LR, X, y, scoring=scoring))</span><br><span class="line"><span class="comment">#getting required score methods</span></span><br><span class="line">LR_report = LR_report.iloc[:,<span class="number">2</span>:]</span><br><span class="line"><span class="comment">#naming the columns</span></span><br><span class="line">LR_report.columns = scoring</span><br><span class="line"><span class="comment">#naming the index</span></span><br><span class="line">LR_report.index = index</span><br><span class="line"></span><br><span class="line"><span class="comment">#final result collection</span></span><br><span class="line">model_1 = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#Classification report</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Clasification report:\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(LR_report,<span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mean Scores:\n&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> score <span class="keyword">in</span> scoring:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\t<span class="subst">&#123;score&#125;</span>: <span class="subst">&#123;<span class="built_in">round</span>(LR_report[score].mean(),<span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line">    model_1.append(<span class="built_in">round</span>(LR_report[score].mean(),<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><code>LR = LogisticRegression()</code>: 创建一个逻辑回归模型的实例。</li>
<li><code>LR_report = pd.DataFrame(cross_validate(LR, X, y, scoring=scoring))</code>: 使用交叉验证来评估模型的性能。<code>cross_validate</code> 函数返回一个字典，其中包含每次交叉验证的结果。这个字典被转换成一个 DataFrame。</li>
<li><code>LR_report = LR_report.iloc[:,2:]</code>: 选择DataFrame 中的一部分列。</li>
<li><code>LR_report.columns = scoring</code> 和 <code>LR_report.index = index</code>: 设置了DataFrame 的列名和行名。</li>
<li><code>model_1 = []</code>: 创建一个空列表，用于收集模型的平均分数。</li>
<li>最后，打印每个评分指标的结果，并计算它们的平均值。这些平均值被添加到了 <code>model_1</code> 列表中。</li>
</ol>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042305650.png" alt="image.png"></p>
<p>预测结果保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample prediction for csv file</span></span><br><span class="line">LR.fit(X_train, y_train)<span class="comment">#training</span></span><br><span class="line">y_predict = LR.predict(X_test)<span class="comment">#predicting</span></span><br><span class="line">Logistic_Regression=pd.DataFrame(&#123;<span class="string">&#x27;y_test&#x27;</span>:y_test,<span class="string">&#x27;prediction&#x27;</span>:y_predict&#125;)<span class="comment">#df for camparison</span></span><br><span class="line">Logistic_Regression.to_csv(<span class="string">&quot;Logistic Regression.csv&quot;</span>)<span class="comment">#export to csv     </span></span><br></pre></td></tr></table></figure>



<h4 id="3-3-决策树分类器（Decision-Tree-Classifier）"><a href="#3-3-决策树分类器（Decision-Tree-Classifier）" class="headerlink" title="3.3 决策树分类器（Decision Tree Classifier）"></a>3.3 决策树分类器（Decision Tree Classifier）</h4><p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model</span></span><br><span class="line">DT = DecisionTreeClassifier()</span><br><span class="line"><span class="comment">#cross validation</span></span><br><span class="line">DT_report = pd.DataFrame(cross_validate(DT, X, y, scoring=scoring))</span><br><span class="line"><span class="comment">#getting required score methods</span></span><br><span class="line">DT_report = DT_report.iloc[:,<span class="number">2</span>:]</span><br><span class="line"><span class="comment">#naming the columns</span></span><br><span class="line">DT_report.columns = scoring</span><br><span class="line"><span class="comment">#naming the index</span></span><br><span class="line">DT_report.index = index</span><br><span class="line"></span><br><span class="line"><span class="comment">#final result collection</span></span><br><span class="line">model_2 = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#Classification report</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Clasification report:\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(DT_report,<span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mean Scores:\n&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> score <span class="keyword">in</span> scoring:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\t<span class="subst">&#123;score&#125;</span>: <span class="subst">&#123;<span class="built_in">round</span>(DT_report[score].mean(),<span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line">    model_2.append(<span class="built_in">round</span>(DT_report[score].mean(),<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><code>DT = DecisionTreeClassifier()</code>: 创建一个决策树分类器的实例。</li>
<li><code>DT_report = pd.DataFrame(cross_validate(DT, X, y, scoring=scoring))</code>: 使用交叉验证来评估模型的性能。<code>cross_validate</code> 函数返回一个字典，其中包含了每次交叉验证的结果。这个字典被转换成一个 DataFrame。</li>
<li><code>DT_report = DT_report.iloc[:,2:]</code>: 选择 DataFrame 中的一部分列。</li>
<li><code>DT_report.columns = scoring</code> 和 <code>DT_report.index = index</code>: 设置 DataFrame 的列名和行名。</li>
<li><code>model_2 = []</code>: 创建一个空列表，用于收集模型的平均分数。</li>
<li>最后，打印每个评分指标的结果，并计算它们的平均值。这些平均值被添加到了 <code>model_2</code> 列表中。</li>
</ol>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042313374.png" alt="image.png"></p>
<p>预测结果保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample prediction for csv file</span></span><br><span class="line">DT.fit(X_train, y_train)<span class="comment">#training</span></span><br><span class="line">y_predict = DT.predict(X_test)<span class="comment">#predicting</span></span><br><span class="line">Decision_Tree=pd.DataFrame(&#123;<span class="string">&#x27;y_test&#x27;</span>:y_test,<span class="string">&#x27;prediction&#x27;</span>:y_predict&#125;)<span class="comment">#df for camparison</span></span><br><span class="line">Decision_Tree.to_csv(<span class="string">&quot;Decision Tree.csv&quot;</span>)<span class="comment">#export to csv     </span></span><br></pre></td></tr></table></figure>


<h4 id="3-4-随机森林回归器（Random-Forest-Regressor）"><a href="#3-4-随机森林回归器（Random-Forest-Regressor）" class="headerlink" title="3.4 随机森林回归器（Random Forest Regressor）"></a>3.4 随机森林回归器（Random Forest Regressor）</h4><p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model</span></span><br><span class="line">RF = RandomForestClassifier()</span><br><span class="line"><span class="comment">#cross validation</span></span><br><span class="line">RF_report = pd.DataFrame(cross_validate(RF, X, y, scoring=scoring))</span><br><span class="line"><span class="comment">#getting required score methods</span></span><br><span class="line">RF_report = RF_report.iloc[:,<span class="number">2</span>:]</span><br><span class="line"><span class="comment">#naming the columns</span></span><br><span class="line">RF_report.columns = scoring</span><br><span class="line"><span class="comment">#naming the index</span></span><br><span class="line">RF_report.index = index</span><br><span class="line"></span><br><span class="line"><span class="comment">#final result collection</span></span><br><span class="line">model_3 = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#Classification report</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Clasification report:\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(RF_report,<span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mean Scores:\n&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> score <span class="keyword">in</span> scoring:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\t<span class="subst">&#123;score&#125;</span>: <span class="subst">&#123;<span class="built_in">round</span>(RF_report[score].mean(),<span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line">    model_3.append(<span class="built_in">round</span>(RF_report[score].mean(),<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><code>RF = RandomForestClassifier()</code>: 创建一个随机森林分类器的实例。</li>
<li><code>RF_report = pd.DataFrame(cross_validate(RF, X, y, scoring=scoring))</code>: 使用交叉验证来评估模型的性能。<code>cross_validate</code> 函数返回一个字典，其中包含每次交叉验证的结果。这个字典被转换成一个 DataFrame。</li>
<li><code>RF_report = RF_report.iloc[:,2:]</code>: 选择 DataFrame 中的一部分列。 </li>
<li><code>RF_report.columns = scoring</code> 和 <code>RF_report.index = index</code>: 设置DataFrame 的列名和行名。</li>
<li><code>model_3 = []</code>: 创建一个空列表，用于收集模型的平均分数。</li>
<li>最后，这段代码打印每个评分指标的结果，并计算它们的平均值。这些平均值被添加到了 <code>model_3</code>列表中。</li>
</ol>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042314553.png" alt="image.png"></p>
<p>预测结果保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample prediction for csv file</span></span><br><span class="line">RF.fit(X_train, y_train)<span class="comment">#training</span></span><br><span class="line">y_predict = RF.predict(X_test)<span class="comment">#predicting</span></span><br><span class="line">Random_Forest = pd.DataFrame(&#123;<span class="string">&#x27;y_test&#x27;</span>:y_test,<span class="string">&#x27;prediction&#x27;</span>:y_predict&#125;)<span class="comment">#df for camparison</span></span><br><span class="line">Random_Forest.to_csv(<span class="string">&quot;Random Forest.csv&quot;</span>)<span class="comment">#export to csv     </span></span><br></pre></td></tr></table></figure>

<h4 id="3-5-XGBoost分类器（XGBoost-Classifier）"><a href="#3-5-XGBoost分类器（XGBoost-Classifier）" class="headerlink" title="3.5 XGBoost分类器（XGBoost Classifier）"></a>3.5 XGBoost分类器（XGBoost Classifier）</h4><p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model</span></span><br><span class="line">XGB = XGBClassifier()</span><br><span class="line"><span class="comment">#cross validation</span></span><br><span class="line">XGB_report = pd.DataFrame(cross_validate(XGB, X, y, scoring=scoring))</span><br><span class="line"><span class="comment">#getting required score methods</span></span><br><span class="line">XGB_report = XGB_report.iloc[:,<span class="number">2</span>:]</span><br><span class="line"><span class="comment">#naming the columns</span></span><br><span class="line">XGB_report.columns = scoring</span><br><span class="line"><span class="comment">#naming the index</span></span><br><span class="line">XGB_report.index = index</span><br><span class="line"></span><br><span class="line"><span class="comment">#final result collection</span></span><br><span class="line">model_4 = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#Classification report</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Clasification report:\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(XGB_report,<span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mean Scores:\n&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> score <span class="keyword">in</span> scoring:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\t<span class="subst">&#123;score&#125;</span>: <span class="subst">&#123;<span class="built_in">round</span>(XGB_report[score].mean(),<span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line">    model_4.append(<span class="built_in">round</span>(XGB_report[score].mean(),<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><code>XGB = XGBClassifier()</code>: 创建一个XGBoost分类器的实例。</li>
<li><code>XGB_report = pd.DataFrame(cross_validate(XGB, X, y, scoring=scoring))</code>: 使用交叉验证来评估模型的性能。<code>cross_validate</code> 函数返回一个字典，其中包含每次交叉验证的结果。这个字典被转换成一个 DataFrame。</li>
<li><code>XGB_report = XGB_report.iloc[:,2:]</code>:选择 DataFrame 中的一部分列。</li>
<li><code>XGB_report.columns = scoring</code> 和 <code>XGB_report.index = index</code>: 设置DataFrame 的列名和行名。</li>
<li><code>model_4 = []</code>: 创建一个空列表，用于收集模型的平均分数。</li>
<li>最后，这段代码打印每个评分指标的结果，并计算它们的平均值。这些平均值被添加到了 <code>model_4</code> 列表中。</li>
</ol>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042319590.png" alt="image.png"></p>
<p>预测结果保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample prediction for csv file</span></span><br><span class="line">XGB.fit(X_train, y_train)<span class="comment">#training</span></span><br><span class="line">y_predict = XGB.predict(X_test)<span class="comment">#predicting</span></span><br><span class="line">XGBoost=pd.DataFrame(&#123;<span class="string">&#x27;y_test&#x27;</span>:y_test,<span class="string">&#x27;prediction&#x27;</span>:y_predict&#125;)<span class="comment">#df for camparison</span></span><br><span class="line">XGBoost.to_csv(<span class="string">&quot;XGBoost.csv&quot;</span>)<span class="comment">#export to csv     </span></span><br></pre></td></tr></table></figure>

<h4 id="3-6-支持向量机（Support-Vector-Machines）"><a href="#3-6-支持向量机（Support-Vector-Machines）" class="headerlink" title="3.6 支持向量机（Support Vector Machines）"></a>3.6 支持向量机（Support Vector Machines）</h4><p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model</span></span><br><span class="line">SVM = SVC(kernel = <span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line"><span class="comment">#cross validation</span></span><br><span class="line">SVM_report = pd.DataFrame(cross_validate(SVM, X, y, scoring=scoring))</span><br><span class="line"><span class="comment">#getting required score methods</span></span><br><span class="line">SVM_report = SVM_report.iloc[:,<span class="number">2</span>:]</span><br><span class="line"><span class="comment">#naming the columns</span></span><br><span class="line">SVM_report.columns = scoring</span><br><span class="line"><span class="comment">#naming the index</span></span><br><span class="line">SVM_report.index = index</span><br><span class="line"></span><br><span class="line"><span class="comment">#final result collection</span></span><br><span class="line">model_5 = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#Classification report</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Clasification report:\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(SVM_report,<span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mean Scores:\n&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> score <span class="keyword">in</span> scoring:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\t<span class="subst">&#123;score&#125;</span>: <span class="subst">&#123;<span class="built_in">round</span>(SVM_report[score].mean(),<span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line">    model_5.append(<span class="built_in">round</span>(SVM_report[score].mean(),<span class="number">4</span>))</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><code>SVM = SVC(kernel = &#39;linear&#39;)</code>: 创建一个支持向量机的实例。这里使用的是线性核函数。</li>
<li><code>SVM_report = pd.DataFrame(cross_validate(SVM, X, y, scoring=scoring))</code>: 使用交叉验证来评估模型的性能。<code>cross_validate</code> 函数返回一个字典，其中包含每次交叉验证的结果。这个字典被转换成一个 DataFrame。</li>
<li><code>SVM_report = SVM_report.iloc[:,2:]</code>:选择 DataFrame 中的一部分列。</li>
<li><code>SVM_report.columns = scoring</code> 和 <code>SVM_report.index = index</code>: 设置DataFrame 的列名和行名。</li>
<li><code>model_5 = []</code>: 这行代码创建一个空列表，用于收集模型的平均分数。</li>
<li>最后，这段代码打印每个评分指标的结果，并计算它们的平均值。这些平均值被添加到了 <code>model_5</code> 列表中。</li>
</ol>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042320106.png" alt="image.png"></p>
<p>预测结果保存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sample prediction for csv file</span></span><br><span class="line">SVM.fit(X_train, y_train)<span class="comment">#training</span></span><br><span class="line">y_predict = SVM.predict(X_test)<span class="comment">#predicting</span></span><br><span class="line">Support_Vector_Machines=pd.DataFrame(&#123;<span class="string">&#x27;y_test&#x27;</span>:y_test,<span class="string">&#x27;prediction&#x27;</span>:y_predict&#125;)<span class="comment">#df for camparison</span></span><br><span class="line">Support_Vector_Machines.to_csv(<span class="string">&quot;Support Vector Machines.csv&quot;</span>)<span class="comment">#export to csv     </span></span><br></pre></td></tr></table></figure>


<h1 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h1><p>通过上述步骤，获得了上述模型的相关性能，现在对它们进行比较，将它们并排显示，并比较不同模型的表现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Models = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;Logistic Regression&#x27;</span>: np.array(model_1),</span><br><span class="line">    <span class="string">&#x27;Decision Tree&#x27;</span>: np.array(model_2),</span><br><span class="line">    <span class="string">&#x27;Random Forest&#x27;</span>: np.array(model_3),</span><br><span class="line">    <span class="string">&#x27;XGBoost&#x27;</span>: np.array(model_4),</span><br><span class="line">    <span class="string">&#x27;Support Vector Machines&#x27;</span>: np.array(model_5)&#125;)</span><br><span class="line">Models.index = scoring</span><br><span class="line"><span class="comment">#Models.sort_values(by=&#x27;Score&#x27;, ascending=False)</span></span><br><span class="line">Models</span><br></pre></td></tr></table></figure>
<p>输出：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306042328667.png" alt="image.png"><br>由以上报告可以得出，在本数据集上，Logistic Regression模型以及XGBoost模型表现得较为优秀。</p>
<p>具体排名：<br><strong>ranking:</strong><br><strong>accuracy</strong> - 1. XGBoost 2. Logistic Regression 3. Random Forest 4. Decision Tree 5. Support Vector Machines</p>
<p><strong>precision</strong> - 1. XGBoost 2. Logistic Regression 3. Random Forest 4. Decision Tree 5. Support Vector Machines</p>
<p><strong>recall</strong> - 1. Logistic Regression 2. Random Forest 3. Decision Tree 4. XGBoost     5. Support Vector Machines  </p>
<p><strong>f1</strong> - 1. XGBoost 2. Logistic Regression 3. Random Forest 4. Decision Tree 5. Support Vector Machines</p>
<p><strong>final ranking:</strong></p>
<ol>
<li>XGBoost</li>
<li>Logistic Regression</li>
<li>Random Forest</li>
<li>Decision Tree</li>
<li>Support Vector Machines</li>
</ol>
<h1 id="五、实验分析"><a href="#五、实验分析" class="headerlink" title="五、实验分析"></a>五、实验分析</h1><p>此次实验，针对本数据集使用了多个模型进行训练以及评估，同时也采用了包括accuracy、precision、recall、f1等多种模型评估指标，使得本次实验的模型对比较为全面，也更为严谨。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2023/06/5879215f93d2.html">http://example.com/2023/06/5879215f93d2.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2023/06/1411b53ef5dd.html"><i class="fa fa-chevron-left">  </i><span></span></a></div><div class="next-post pull-right"><a href="/2023/06/d3cad4e9c8fb.html"><span></span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2023 By John Doe</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>