<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><title>Hexo</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.3.0'
} </script><meta name="generator" content="Hexo 6.3.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84"><span class="toc-number">1.</span> <span class="toc-text">一、实验目的</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%8E%9F%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">二、实验原理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9%E5%92%8C%E6%AD%A5%E9%AA%A4"><span class="toc-number">3.</span> <span class="toc-text">三、实验内容和步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9"><span class="toc-number">3.1.</span> <span class="toc-text">(1)实验内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4"><span class="toc-number">3.2.</span> <span class="toc-text">(2)实验步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%B9%B6%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E6%9C%89%E7%BC%BA%E7%9C%81%E5%80%BC"><span class="toc-number">3.2.1.</span> <span class="toc-text">1. 导入数据并判断是否有缺省值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.2.2.</span> <span class="toc-text">2. 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="toc-number">3.2.3.</span> <span class="toc-text">3. 模型训练与评估</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-SVM-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">3.2.3.1.</span> <span class="toc-text">3.1 SVM(支持向量机)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Naive-Bayes-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">3.2.3.2.</span> <span class="toc-text">3.2 Naive Bayes(朴素贝叶斯)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-Logistic-Regression-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">3.2.3.3.</span> <span class="toc-text">3.3 Logistic Regression(逻辑回归)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-Decision-Tree-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">3.2.3.4.</span> <span class="toc-text">3.4 Decision Tree(决策树)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-Random-Forest-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">3.2.3.5.</span> <span class="toc-text">3.5 Random Forest(随机森林)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-LightGBM"><span class="toc-number">3.2.3.6.</span> <span class="toc-text">3.6 LightGBM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-7-XGBoost"><span class="toc-number">3.2.3.7.</span> <span class="toc-text">3.7 XGBoost</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">4.</span> <span class="toc-text">四、实验结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%AE%9E%E9%AA%8C%E5%88%86%E6%9E%90"><span class="toc-number">5.</span> <span class="toc-text">五、实验分析</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">John Doe</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">51</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">3</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">37</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Hexo</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right"></span></div><div id="post-info"><div id="post-title">No title</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-06-04</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">数据挖掘</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/">实验报告</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="一、实验目的"><a href="#一、实验目的" class="headerlink" title="一、实验目的"></a>一、实验目的</h1><p>(1) 编写程序，实现心脏病的预测模型的建立。<br>(2) 对多个模型，依据混淆矩阵进行评估度量。<br>(3) 写出实验报告。</p>
<h1 id="二、实验原理"><a href="#二、实验原理" class="headerlink" title="二、实验原理"></a>二、实验原理</h1><p>(1)数据预处理：首先，对心脏病数据集进行数据预处理，包括特征选择、缺失值处理、数据标准化等操作，以准备数据集用于模型训练和测试。 </p>
<p>(2)模型训练：选择了多个机器学习算法，包括SVM、朴素贝叶斯、逻辑回归、决策树、随机森林、LightGBM和XGBoost。对于每个算法，使用训练集对模型进行训练，调整算法参数以获得最佳性能。  </p>
<p>(3)模型评估：使用测试集对训练好的模型进行预测，并计算评估指标：混淆矩阵。  </p>
<p>(4)结果分析：根据实验结果，对不同算法的预测性能进行比较和分析，以确定哪种算法在心脏病预测任务中表现最好</p>
<h1 id="三、实验内容和步骤"><a href="#三、实验内容和步骤" class="headerlink" title="三、实验内容和步骤"></a>三、实验内容和步骤</h1><h2 id="1-实验内容"><a href="#1-实验内容" class="headerlink" title="(1)实验内容"></a>(1)实验内容</h2><ol>
<li>数据预处理：对心脏病数据集进行特征选择、缺失值处理、数据标准化等预处理操作。</li>
<li>模型训练：使用SVM、朴素贝叶斯、逻辑回归、决策树、随机森林、LightGBM和XGBoost等算法进行模型训练。</li>
<li>模型评估：使用测试集对训练好的模型进行预测，并计算评估指标，混淆矩阵。</li>
<li>结果比较和分析：对不同算法的预测性能进行比较和分析，以确定最佳的模型。</li>
<li>结论：总结实验结果，给出针对心脏病预测任务的最佳模型选择和建议。</li>
</ol>
<h2 id="2-实验步骤"><a href="#2-实验步骤" class="headerlink" title="(2)实验步骤"></a>(2)实验步骤</h2><ol>
<li>导入数据并判断是否有缺省值</li>
<li>数据预处理</li>
<li>模型训练与评估</li>
</ol>
<p>具体步骤：</p>
<h3 id="1-导入数据并判断是否有缺省值"><a href="#1-导入数据并判断是否有缺省值" class="headerlink" title="1. 导入数据并判断是否有缺省值"></a>1. 导入数据并判断是否有缺省值</h3><p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># import warnings filter</span></span><br><span class="line"><span class="keyword">from</span> warnings <span class="keyword">import</span> simplefilter</span><br><span class="line"><span class="comment"># ignore all future warnings</span></span><br><span class="line">simplefilter(action=<span class="string">&#x27;ignore&#x27;</span>, category = FutureWarning)</span><br><span class="line"><span class="comment"># import warnings filter</span></span><br><span class="line"><span class="keyword">from</span> warnings <span class="keyword">import</span> simplefilter</span><br><span class="line"><span class="comment"># ignore all future warnings</span></span><br><span class="line">simplefilter(action=<span class="string">&#x27;ignore&#x27;</span>, category = FutureWarning)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;cleveland.csv&#x27;</span>, header = <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">df.columns = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;cp&#x27;</span>, <span class="string">&#x27;trestbps&#x27;</span>, <span class="string">&#x27;chol&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;fbs&#x27;</span>, <span class="string">&#x27;restecg&#x27;</span>, <span class="string">&#x27;thalach&#x27;</span>, <span class="string">&#x27;exang&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;oldpeak&#x27;</span>, <span class="string">&#x27;slope&#x27;</span>, <span class="string">&#x27;ca&#x27;</span>, <span class="string">&#x27;thal&#x27;</span>, <span class="string">&#x27;target&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### 1 = male, 0 = female</span></span><br><span class="line">df.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<p>输出结果：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306041529195.png" alt="image.png"></p>
<p>查看每个目标阶层的年龄和性别分布<br>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;target&#x27;</span>] = df.target.<span class="built_in">map</span>(&#123;<span class="number">0</span>: <span class="number">0</span>, <span class="number">1</span>: <span class="number">1</span>, <span class="number">2</span>: <span class="number">1</span>, <span class="number">3</span>: <span class="number">1</span>, <span class="number">4</span>: <span class="number">1</span>&#125;)</span><br><span class="line">df[<span class="string">&#x27;sex&#x27;</span>] = df.sex.<span class="built_in">map</span>(&#123;<span class="number">0</span>: <span class="string">&#x27;female&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;male&#x27;</span>&#125;)</span><br><span class="line">df[<span class="string">&#x27;thal&#x27;</span>] = df.thal.fillna(df.thal.mean())</span><br><span class="line">df[<span class="string">&#x27;ca&#x27;</span>] = df.ca.fillna(df.ca.mean())</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># distribution of target vs age</span></span><br><span class="line">sns.set_context(<span class="string">&quot;paper&quot;</span>, font_scale = <span class="number">2</span>, rc = &#123;<span class="string">&quot;font.size&quot;</span>: <span class="number">20</span>,<span class="string">&quot;axes.titlesize&quot;</span>: <span class="number">25</span>,<span class="string">&quot;axes.labelsize&quot;</span>: <span class="number">20</span>&#125;) </span><br><span class="line">sns.catplot(kind = <span class="string">&#x27;count&#x27;</span>, data = df, x = <span class="string">&#x27;age&#x27;</span>, hue = <span class="string">&#x27;target&#x27;</span>, order = df[<span class="string">&#x27;age&#x27;</span>].sort_values().unique())</span><br><span class="line">plt.title(<span class="string">&#x27;Variation of Age for each target class&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306041535081.png" alt="image.png"></p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># barplot of age vs sex with hue = target</span></span><br><span class="line">sns.catplot(kind = <span class="string">&#x27;bar&#x27;</span>, data = df, y = <span class="string">&#x27;age&#x27;</span>, x = <span class="string">&#x27;sex&#x27;</span>, hue = <span class="string">&#x27;target&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Distribution of age vs sex with the target class&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306041537017.png" alt="image.png"></p>
<h3 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">################################## data preprocessing</span></span><br><span class="line">X = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">y = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler <span class="keyword">as</span> ss</span><br><span class="line">sc = ss()</span><br><span class="line">X_train = sc.fit_transform(X_train)</span><br><span class="line">X_test = sc.transform(X_test)</span><br></pre></td></tr></table></figure>
<p><strong>代码解析：</strong></p>
<ol>
<li><p>数据划分：使用<code>train_test_split</code>函数将数据集<code>df</code>划分为训练集和测试集。<code>X</code>是除了最后一列外的所有特征，<code>y</code>是最后一列的目标变量。通过设置<code>test_size</code>参数为0.2，将数据集划分为80%的训练集和20%的测试集。<code>random_state</code>参数用于设定随机种子，以确保每次划分结果的一致性。</p>
</li>
<li><p>特征缩放：使用<code>StandardScaler</code>类进行特征缩放。<code>sc</code>对象是<code>StandardScaler</code>的实例。首先，<code>fit_transform</code>方法在训练集上进行拟合和转换，计算每个特征的均值和标准差，并将训练集进行标准化处理。然后，使用<code>transform</code>方法将测试集按照相同的均值和标准差进行标准化处理。</p>
</li>
</ol>
<p>通过数据划分和特征缩放，可以将原始数据集划分为训练集和测试集，并对特征进行标准化处理，以便在后续的模型训练和评估中使用。这些步骤有助于确保模型在相同的数据范围内进行训练和测试，提高模型的性能和泛化能力。</p>
<h3 id="3-模型训练与评估"><a href="#3-模型训练与评估" class="headerlink" title="3. 模型训练与评估"></a>3. 模型训练与评估</h3><p>应用以下模型进行训练，并且使用混淆矩阵进行模型评估与度量<br>以下模型将具体使用<br>$$accuracy&#x3D;\frac{TP+TN}{TP+TN+FP+FN}$$<br>作为模型的评估度量<br>混淆矩阵如下图所示：<br><img src="https://images-1314484890.cos.ap-nanjing.myqcloud.com/Obsidian/202306041552306.png" alt="image.png"></p>
<h4 id="3-1-SVM-支持向量机"><a href="#3-1-SVM-支持向量机" class="headerlink" title="3.1 SVM(支持向量机)"></a>3.1 SVM(支持向量机)</h4><p><strong>代码解析：</strong></p>
<ol>
<li>模型训练：使用<code>SVC</code>类创建一个SVM分类器对象。<code>kernel=&#39;rbf&#39;</code>参数指定了使用径向基函数作为核函数。然后，使用<code>fit</code>方法将训练集的特征<code>X_train</code>和标签<code>y_train</code>作为输入，对分类器进行训练。</li>
<li>预测测试集结果：使用训练好的分类器对测试集的特征<code>X_test</code>进行预测，使用<code>predict</code>方法得到预测的类别标签<code>y_pred</code>。</li>
<li>混淆矩阵：使用<code>confusion_matrix</code>函数计算测试集预测结果的混淆矩阵。</li>
<li>预测训练集结果和混淆矩阵：类似地，使用训练好的分类器对训练集的特征<code>X_train</code>进行预测，得到预测的类别标签<code>y_pred_train</code>。然后使用<code>confusion_matrix</code>函数计算训练集预测结果的混淆矩阵。</li>
</ol>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########################################   SVM   #############################################################</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">classifier = SVC(kernel = <span class="string">&#x27;rbf&#x27;</span>)</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predicting the Test set results</span></span><br><span class="line">y_pred = classifier.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm_test = confusion_matrix(y_pred, y_test)</span><br><span class="line"></span><br><span class="line">y_pred_train = classifier.predict(X_train)</span><br><span class="line">cm_train = confusion_matrix(y_pred_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for training set for svm = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_train[<span class="number">0</span>][<span class="number">0</span>] + cm_train[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for test set for svm = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_test[<span class="number">0</span>][<span class="number">0</span>] + cm_test[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_test)))</span><br></pre></td></tr></table></figure>
<p>输出结果：<br><code>Accuracy for training set for svm = 0.9256198347107438 </code><br><code>Accuracy for test set for svm = 0.8032786885245902</code></p>
<h4 id="3-2-Naive-Bayes-朴素贝叶斯"><a href="#3-2-Naive-Bayes-朴素贝叶斯" class="headerlink" title="3.2 Naive Bayes(朴素贝叶斯)"></a>3.2 Naive Bayes(朴素贝叶斯)</h4><p><strong>代码解析：</strong></p>
<ol>
<li>数据准备：使用<code>df.iloc[:, :-1].values</code>将数据集中除了最后一列之外的所有特征赋值给<code>X</code>，将最后一列的标签赋值给<code>y</code>。</li>
<li>数据划分：使用<code>train_test_split</code>函数将数据集划分为训练集和测试集。通过设置<code>test_size</code>参数为0.2，将数据集划分为80%的训练集和20%的测试集。<code>random_state</code>参数用于设定随机种子，以确保每次划分结果的一致性。划分后的训练集特征赋值给<code>X_train</code>，训练集标签赋值给<code>y_train</code>，测试集特征赋值给<code>X_test</code>，测试集标签赋值给<code>y_test</code>。</li>
<li>模型训练：使用<code>GaussianNB</code>类创建一个朴素贝叶斯分类器对象，即高斯朴素贝叶斯分类器。然后，使用<code>fit</code>方法将训练集的特征<code>X_train</code>和标签<code>y_train</code>作为输入，对分类器进行训练。</li>
<li>预测测试集结果：使用训练好的分类器对测试集的特征<code>X_test</code>进行预测，使用<code>predict</code>方法得到预测的类别标签<code>y_pred</code>。</li>
<li>混淆矩阵：使用<code>confusion_matrix</code>函数计算测试集预测结果的混淆矩阵。</li>
<li>预测训练集结果和混淆矩阵：类似地，使用训练好的分类器对训练集的特征<code>X_train</code>进行预测，得到预测的类别标签<code>y_pred_train</code>。然后使用<code>confusion_matrix</code>函数计算训练集预测结果的混淆矩阵。</li>
</ol>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########################################   Naive Bayes  #############################################################</span></span><br><span class="line">X = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">y = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line">classifier = GaussianNB()</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predicting the Test set results</span></span><br><span class="line">y_pred = classifier.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm_test = confusion_matrix(y_pred, y_test)</span><br><span class="line"></span><br><span class="line">y_pred_train = classifier.predict(X_train)</span><br><span class="line">cm_train = confusion_matrix(y_pred_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for training set for Naive Bayes = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_train[<span class="number">0</span>][<span class="number">0</span>] + cm_train[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for test set for Naive Bayes = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_test[<span class="number">0</span>][<span class="number">0</span>] + cm_test[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_test)))</span><br><span class="line"></span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>输出结果：<br><code>Accuracy for training set for Naive Bayes = 0.8677685950413223</code><br><code>Accuracy for test set for Naive Bayes = 0.7868852459016393</code></p>
<h4 id="3-3-Logistic-Regression-逻辑回归"><a href="#3-3-Logistic-Regression-逻辑回归" class="headerlink" title="3.3 Logistic Regression(逻辑回归)"></a>3.3 Logistic Regression(逻辑回归)</h4><p><strong>代码解析：</strong></p>
<ol>
<li>数据准备：使用<code>df.iloc[:, :-1].values</code>将数据集中除了最后一列之外的所有特征赋值给<code>X</code>，将最后一列的标签赋值给<code>y</code>。</li>
<li>数据划分：使用<code>train_test_split</code>函数将数据集划分为训练集和测试集。通过设置<code>test_size</code>参数为0.2，将数据集划分为80%的训练集和20%的测试集。<code>random_state</code>参数用于设定随机种子，以确保每次划分结果的一致性。划分后的训练集特征赋值给<code>X_train</code>，训练集标签赋值给<code>y_train</code>，测试集特征赋值给<code>X_test</code>，测试集标签赋值给<code>y_test</code>。</li>
<li>模型训练：使用<code>LogisticRegression</code>类创建一个逻辑回归分类器对象。然后，使用<code>fit</code>方法将训练集的特征<code>X_train</code>和标签<code>y_train</code>作为输入，对分类器进行训练。</li>
<li>预测测试集结果：使用训练好的分类器对测试集的特征<code>X_test</code>进行预测，使用<code>predict</code>方法得到预测的类别标签<code>y_pred</code>。</li>
<li>混淆矩阵：使用<code>confusion_matrix</code>函数计算测试集预测结果的混淆矩阵。</li>
<li>预测训练集结果和混淆矩阵：类似地，使用训练好的分类器对训练集的特征<code>X_train</code>进行预测，得到预测的类别标签<code>y_pred_train</code>。然后使用<code>confusion_matrix</code>函数计算训练集预测结果的混淆矩阵。</li>
</ol>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#########################################   Logistic Regression  #############################################################</span></span><br><span class="line">X = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">y = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">classifier = LogisticRegression()</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predicting the Test set results</span></span><br><span class="line">y_pred = classifier.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm_test = confusion_matrix(y_pred, y_test)</span><br><span class="line"></span><br><span class="line">y_pred_train = classifier.predict(X_train)</span><br><span class="line">cm_train = confusion_matrix(y_pred_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for training set for Logistic Regression = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_train[<span class="number">0</span>][<span class="number">0</span>] + cm_train[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for test set for Logistic Regression = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_test[<span class="number">0</span>][<span class="number">0</span>] + cm_test[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_test)))</span><br><span class="line"></span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>输出结果：<br><code>Accuracy for training set for Logistic Regression = 0.8677685950413223</code><br><code>Accuracy for test set for Logistic Regression = 0.8032786885245902</code></p>
<h4 id="3-4-Decision-Tree-决策树"><a href="#3-4-Decision-Tree-决策树" class="headerlink" title="3.4 Decision Tree(决策树)"></a>3.4 Decision Tree(决策树)</h4><p><strong>代码解析：</strong></p>
<ol>
<li>数据准备：使用<code>df.iloc[:, :-1].values</code>将数据集中除了最后一列之外的所有特征赋值给<code>X</code>，将最后一列的标签赋值给<code>y</code>。</li>
<li>数据划分：使用<code>train_test_split</code>函数将数据集划分为训练集和测试集。通过设置<code>test_size</code>参数为0.2，将数据集划分为80%的训练集和20%的测试集。<code>random_state</code>参数用于设定随机种子，以确保每次划分结果的一致性。划分后的训练集特征赋值给<code>X_train</code>，训练集标签赋值给<code>y_train</code>，测试集特征赋值给<code>X_test</code>，测试集标签赋值给<code>y_test</code>。</li>
<li>模型训练：使用<code>DecisionTreeClassifier</code>类创建一个决策树分类器对象。然后，使用<code>fit</code>方法将训练集的特征<code>X_train</code>和标签<code>y_train</code>作为输入，对分类器进行训练。</li>
<li>预测测试集结果：使用训练好的分类器对测试集的特征<code>X_test</code>进行预测，使用<code>predict</code>方法得到预测的类别标签<code>y_pred</code>。</li>
<li>混淆矩阵：使用<code>confusion_matrix</code>函数计算测试集预测结果的混淆矩阵。</li>
<li>预测训练集结果和混淆矩阵：类似地，使用训练好的分类器对训练集的特征<code>X_train</code>进行预测，得到预测的类别标签<code>y_pred_train</code>。然后使用<code>confusion_matrix</code>函数计算训练集预测结果的混淆矩阵。</li>
</ol>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########################################   Decision Tree  #############################################################</span></span><br><span class="line">X = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">y = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">classifier = DecisionTreeClassifier()</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predicting the Test set results</span></span><br><span class="line">y_pred = classifier.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm_test = confusion_matrix(y_pred, y_test)</span><br><span class="line"></span><br><span class="line">y_pred_train = classifier.predict(X_train)</span><br><span class="line">cm_train = confusion_matrix(y_pred_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for training set for Decision Tree = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_train[<span class="number">0</span>][<span class="number">0</span>] + cm_train[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for test set for Decision Tree = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_test[<span class="number">0</span>][<span class="number">0</span>] + cm_test[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_test)))</span><br><span class="line"></span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>输出结果：<br><code>Accuracy for training set for Decision Tree = 1.0</code><br><code>Accuracy for test set for Decision Tree = 0.8032786885245902</code></p>
<h4 id="3-5-Random-Forest-随机森林"><a href="#3-5-Random-Forest-随机森林" class="headerlink" title="3.5 Random Forest(随机森林)"></a>3.5 Random Forest(随机森林)</h4><p><strong>代码解析：</strong></p>
<ol>
<li>数据准备：使用<code>df.iloc[:, :-1].values</code>将数据集中除了最后一列之外的所有特征赋值给<code>X</code>，将最后一列的标签赋值给<code>y</code>。</li>
<li>数据划分：使用<code>train_test_split</code>函数将数据集划分为训练集和测试集。通过设置<code>test_size</code>参数为0.2，将数据集划分为80%的训练集和20%的测试集。<code>random_state</code>参数用于设定随机种子，以确保每次划分结果的一致性。划分后的训练集特征赋值给<code>X_train</code>，训练集标签赋值给<code>y_train</code>，测试集特征赋值给<code>X_test</code>，测试集标签赋值给<code>y_test</code>。</li>
<li>模型训练：使用<code>RandomForestClassifier</code>类创建一个随机森林分类器对象。通过设置<code>n_estimators</code>参数为10，指定随机森林中树的数量为10。然后，使用<code>fit</code>方法将训练集的特征<code>X_train</code>和标签<code>y_train</code>作为输入，对分类器进行训练。</li>
<li>预测测试集结果：使用训练好的分类器对测试集的特征<code>X_test</code>进行预测，使用<code>predict</code>方法得到预测的类别标签<code>y_pred</code>。</li>
<li>混淆矩阵：使用<code>confusion_matrix</code>函数计算测试集预测结果的混淆矩阵。</li>
<li>预测训练集结果和混淆矩阵：类似地，使用训练好的分类器对训练集的特征<code>X_train</code>进行预测，得到预测的类别标签<code>y_pred_train</code>。然后使用<code>confusion_matrix</code>函数计算训练集预测结果的混淆矩阵。</li>
</ol>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########################################  Random Forest  #############################################################</span></span><br><span class="line">X = df.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">y = df.iloc[:, -<span class="number">1</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">classifier = RandomForestClassifier(n_estimators = <span class="number">10</span>)</span><br><span class="line">classifier.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predicting the Test set results</span></span><br><span class="line">y_pred = classifier.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm_test = confusion_matrix(y_pred, y_test)</span><br><span class="line"></span><br><span class="line">y_pred_train = classifier.predict(X_train)</span><br><span class="line">cm_train = confusion_matrix(y_pred_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for training set for Random Forest = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_train[<span class="number">0</span>][<span class="number">0</span>] + cm_train[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for test set for Random Forest = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_test[<span class="number">0</span>][<span class="number">0</span>] + cm_test[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_test)))</span><br><span class="line"></span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>输出结果：<br><code>Accuracy for training set for Random Forest = 0.9834710743801653</code><br><code>Accuracy for test set for Random Forest = 0.7049180327868853</code></p>
<h4 id="3-6-LightGBM"><a href="#3-6-LightGBM" class="headerlink" title="3.6 LightGBM"></a>3.6 LightGBM</h4><p><strong>代码解析：</strong></p>
<ol>
<li>数据准备：使用<code>lgb.Dataset</code>函数将训练集的特征<code>X_train</code>和标签<code>y_train</code>组成LightGBM需要的数据集对象<code>d_train</code>。</li>
<li>参数设置：定义一个空字典<code>params</code>用于设置LightGBM的参数。</li>
<li>模型训练：使用<code>lgb.train</code>函数训练LightGBM模型。传入参数<code>params</code>表示模型的参数设置，<code>d_train</code>表示训练数据集，<code>100</code>表示训练的轮数（迭代次数）。</li>
<li>预测测试集结果：使用训练好的模型对测试集的特征<code>X_test</code>进行预测，得到预测的概率值<code>y_pred</code>。</li>
<li>二值化处理：根据设定的阈值（0.5），将概率值转换为二进制的类别标签。概率值大于等于0.5的被划分为类别1，小于0.5的被划分为类别0。</li>
<li>混淆矩阵：使用<code>confusion_matrix</code>函数计算测试集预测结果的混淆矩阵。</li>
<li>预测训练集结果和混淆矩阵：类似地，使用训练好的模型对训练集的特征<code>X_train</code>进行预测，得到预测的概率值<code>y_pred_train</code>。然后使用<code>confusion_matrix</code>函数计算训练集预测结果的混淆矩阵。</li>
</ol>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment"># applying lightGBM</span></span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"></span><br><span class="line">d_train = lgb.Dataset(X_train, label = y_train)</span><br><span class="line">params = &#123;&#125;</span><br><span class="line"></span><br><span class="line">clf = lgb.train(params, d_train, <span class="number">100</span>)</span><br><span class="line"><span class="comment">#Prediction</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"><span class="comment">#convert into binary values</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y_pred)):</span><br><span class="line">    <span class="keyword">if</span> y_pred[i]&gt;= <span class="number">0.5</span>:       <span class="comment"># setting threshold to .5</span></span><br><span class="line">       y_pred[i]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">       y_pred[i]=<span class="number">0</span></span><br><span class="line">       </span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm_test = confusion_matrix(y_pred, y_test)</span><br><span class="line"></span><br><span class="line">y_pred_train = clf.predict(X_train)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y_pred_train)):</span><br><span class="line">    <span class="keyword">if</span> y_pred_train[i]&gt;= <span class="number">0.5</span>:       <span class="comment"># setting threshold to .5</span></span><br><span class="line">       y_pred_train[i]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">       y_pred_train[i]=<span class="number">0</span></span><br><span class="line">       </span><br><span class="line">cm_train = confusion_matrix(y_pred_train, y_train)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for training set for LightGBM = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_train[<span class="number">0</span>][<span class="number">0</span>] + cm_train[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for test set for LightGBM = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_test[<span class="number">0</span>][<span class="number">0</span>] + cm_test[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_test)))</span><br><span class="line"></span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>输出结果：<br><code>Accuracy for training set for LightGBN = 0.9958677685950413</code><br><code>Accuracy for test set for LightGBN = 0.7704918032786885</code></p>
<h4 id="3-7-XGBoost"><a href="#3-7-XGBoost" class="headerlink" title="3.7 XGBoost"></a>3.7 XGBoost</h4><p><strong>代码解析：</strong></p>
<ol>
<li>模型训练：使用<code>XGBClassifier</code>类创建一个XGBoost分类器对象<code>xg</code>。然后，使用<code>fit</code>方法将训练集的特征<code>X_train</code>和标签<code>y_train</code>作为输入，对分类器进行训练。</li>
<li>预测测试集结果：使用训练好的分类器对测试集的特征<code>X_test</code>进行预测，得到预测的类别标签<code>y_pred</code>。</li>
<li>混淆矩阵：使用<code>confusion_matrix</code>函数计算测试集预测结果的混淆矩阵。</li>
<li>预测训练集结果和混淆矩阵：类似地，使用训练好的分类器对训练集的特征<code>X_train</code>进行预测，得到预测的类别标签<code>y_pred_train</code>。然后使用<code>confusion_matrix</code>函数计算训练集预测结果的混淆矩阵。</li>
</ol>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment"># applying XGBoost</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#from sklearn.model_selection import train_test_split</span></span><br><span class="line"><span class="comment">#X_train, X_test, y_train, y_test = train_test_split(X, target, test_size = 0.20, random_state = 0)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line">xg = XGBClassifier()</span><br><span class="line">xg.fit(X_train, y_train)</span><br><span class="line">y_pred = xg.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm_test = confusion_matrix(y_pred, y_test)</span><br><span class="line"></span><br><span class="line">y_pred_train = xg.predict(X_train)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y_pred_train)):</span><br><span class="line">    <span class="keyword">if</span> y_pred_train[i]&gt;= <span class="number">0.5</span>:       <span class="comment"># setting threshold to .5</span></span><br><span class="line">       y_pred_train[i]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">       y_pred_train[i]=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">cm_train = confusion_matrix(y_pred_train, y_train)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for training set for XGBoost = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_train[<span class="number">0</span>][<span class="number">0</span>] + cm_train[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy for test set for XGBoost = &#123;&#125;&#x27;</span>.<span class="built_in">format</span>((cm_test[<span class="number">0</span>][<span class="number">0</span>] + cm_test[<span class="number">1</span>][<span class="number">1</span>])/<span class="built_in">len</span>(y_test)))</span><br><span class="line"></span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>输出结果：<br><code>Accuracy for training set for XGBoost = 0.987603305785124 </code>Accuracy for test set for XGBoost &#x3D; 0.7540983606557377&#96;</p>
<h1 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h1><p>通过对上述七个模型进行混淆矩阵评估度量，得到以下实验结果：<br><code>Accuracy for training set for svm = 0.9256198347107438 </code><br><code>Accuracy for test set for svm = 0.8032786885245902</code></p>
<p><code>Accuracy for training set for Naive Bayes = 0.8677685950413223</code><br><code>Accuracy for test set for Naive Bayes = 0.7868852459016393</code></p>
<p><code>Accuracy for training set for Logistic Regression = 0.8677685950413223</code><br><code>Accuracy for test set for Logistic Regression = 0.8032786885245902</code></p>
<p><code>Accuracy for training set for Decision Tree = 1.0</code><br><code>Accuracy for test set for Decision Tree = 0.8032786885245902</code></p>
<p><code>Accuracy for training set for Random Forest = 0.9834710743801653</code><br><code>Accuracy for test set for Random Forest = 0.7049180327868853</code></p>
<p><code>Accuracy for training set for LightGBN = 0.9958677685950413</code><br><code>Accuracy for test set for LightGBN = 0.7704918032786885</code></p>
<p><code>Accuracy for training set for XGBoost = 0.987603305785124 </code>Accuracy for test set for XGBoost &#x3D; 0.7540983606557377&#96;<br>由此可得，在训练集上Decision Tree具有最高的Accuracy；而在测试集上SVM、Logistic Regression、Decision Tree具有最高的Accuracy。</p>
<h1 id="五、实验分析"><a href="#五、实验分析" class="headerlink" title="五、实验分析"></a>五、实验分析</h1><p>此次实验，在多个模型上进行了优劣对比，但在模型的评估度量上只使用了混淆矩阵中的Accuracy，往后可以考虑用更多评估度量标准来对模型进行评估，评估模型的性能和可靠性。以此获得更加全面准确的模型报告。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2023/06/4c83db3bac28.html">http://example.com/2023/06/4c83db3bac28.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2023/06/60d9fd126048.html"><i class="fa fa-chevron-left">  </i><span></span></a></div><div class="next-post pull-right"><a href="/2023/06/6f49b06c6fae.html"><span></span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2023 By John Doe</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>